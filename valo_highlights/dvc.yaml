stages:
  # Stage A: cut out your labeled mini-clips
  extract_segments:
    cmd: bash scripts/extract_segments.sh
    deps:
      - raw_videos
      - annotations/Highlights.csv
      - scripts/extract_segments.sh
    outs:
      - raw_videos/segments

  # Stage B: extract frames from those mini-clips
  extract_frames:
    cmd: .venv/bin/python scripts/extract_frames.py
    deps:
      - raw_videos/segments
      - scripts/extract_frames.py
    outs:
      - frames

  # Stage C: extract audio
  extract_audio:
    cmd: .venv/bin/python scripts/extract_audio.py
    deps:
      - raw_videos/segments
      - scripts/extract_audio.py
    outs:
      - audio

  # Stage D: run OCR on frames
  run_ocr:
    cmd: .venv/bin/python scripts/run_ocr.py
    deps:
      - frames
      - scripts/run_ocr.py
    outs:
      - ocr

  # Stage E: merge OCR into your annotations CSV
  preprocess_ocr:
    cmd: .venv/bin/python scripts/preprocess_ocr.py
    deps:
      - ocr
      - annotations/Highlights.csv
      - scripts/preprocess_ocr.py
    outs:
      - annotations/with_ocr.csv
